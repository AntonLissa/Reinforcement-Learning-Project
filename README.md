# ğŸ§  Reinforcement Learning Project  

This project, developed for the Reinforcement Learning exam, explores **multi-agent survival dynamics**.  
Two groups of agents must **hunt the other to survive**, comparing the effectiveness of **Proximal Policy Optimization (PPO)** versus a **Genetic Algorithm (GA)** for evolving the agentsâ€™ neural networks.  

---

## ğŸ® Simulation Rules  

Each agent can:  
- ğŸ‘€ **See** the 3 closest objects in its field of view âˆŠ [-90Â°, 90Â°]  
- ğŸƒ **Decide linear speed** âˆŠ [-100, 100]  
- ğŸ”„ **Decide angular speed** âˆŠ [-10, 10]  
- ğŸ½ï¸ **Eat** smaller enemies and food  

Environment setup:  
- At every new epoch, **food, obstacles, and agents** are randomly placed.  

---

## ğŸ† Rewards  

- â• **+1** â†’ eating food  
- â• **+2** â†’ eating an enemy  
- â– **-1** â†’ colliding with obstacles or getting eaten  

---

## ğŸ§© Agent Perception  

For the 3 closest objects, each agent perceives:  
- ğŸ¯ **Pointing error** âˆŠ [-0.5, 0.5]  
- ğŸ“ **Distance** âˆŠ [0, 1]  
- âš–ï¸ **Size** âˆŠ {-1, 1}  
- ğŸ‘« **Team ID**  

---

## ğŸ§ª Training Results with Genetic Algorithm

Before training, the agentsâ€™ neural networks have no understanding of signals. They move almost randomly, often colliding with obstacles or changing direction aimlessly:  

<img src="before_training.gif" alt="Before Training" width="500"/>  

After training, most agents learn to **avoid obstacles** and **pursue smaller prey** effectively:  

<img src="result_after_training.gif" alt="After Training" width="500"/>  

---


